{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minodes Fingerprint to Zone Classification Challenge\n",
    "\n",
    "## Introduction\n",
    "\n",
    "At Minodes, we provide insights to retailers using WiFi Analytics. Insights are derived by analysing the location of people's smartphones during their visits.\n",
    "\n",
    "How does it work? We install a set of WiFi routers (so called \"nodes\") with a custom firmware inside or close to the areas to be monitored. Nodes collect the signal strength (`RSSI`) of WiFi probe requests sent regularly by smartphones (so called \"observations\"). Observations are then used to estimate the position of smartphones: Let `RSSI(N,X)` be a function that returns the signal strength of probe request `X` from node `N` (measured in dBm: `0` is strongest signal strength, `-100` is weakest signal strength, extremes are seldomly reached). The set of observations `RSSI(Ni,X)` for all nodes `i` is called \"fingerprint\" and constitutes a radio signature of the smartphone correlated (non linearly) to its position in space (a distance of few meters translates to a signal strength within `[0,-30]`). Stores are manually partitioned into regions (so called \"zones\"), e.g., \"entrance\" and \"checkout area\". Fingerprints and zones are used as features and prediction classes, respectively.\n",
    "\n",
    "Example: Given a store with zones `{A,B}`, nodes `{N1 (inside A), N2 (inside B), N3 (between A and B)}`, we want to estimate the most likely zone where the device that generated probe request `X` is located at that point of time. Intuitively, if `RSSI(N1,X) > RSSI(N3,X)`, the device is probably located in zone `A`. If `RSSI(N3,X)` is the highest, the answer is uncertain and we must look at the relative difference of the values of `RSSI(N1,X), RSSI(N3,X)`. If `RSSI(N2,X) > RSSI(N1,X)`, then most likely the correct zone is `B`.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "This archive contains a dataset in CSV format located at `data/fingerprints_gt_ver3.csv`. The dataset represents a large deployment in a mall (three floors), and has been generated by walking inside each zone with a set of phones, labeling fingerprints manually with their corresponding zone. Nodes are located at the exterior of the entrance of the stores, and not inside the stores. Zones map to shops, aisles, and surrounding areas. The dataset is structured as follows:\n",
    "\n",
    "| fr_observation_time  | fr_values  | fr_mac_address_id | zo_name  |\n",
    "| -------------------- | ---------- | ----------------- | ---------|  \n",
    "| 2015-12-08 10:00:13  | {'9': '-83', '13': '-67', '33': '-62', '101': ...  | 3192369 | Zone 355  |\n",
    "| 2015-12-08 10:00:13  | {'12': '-69', '33': '-61', '128': '-68', '276'...  | 2002427 | Zone 355  |\n",
    "\n",
    "Description of the fields:\n",
    "\n",
    "* `fr_observation_time`: first timestamp in ascending order of the observations aggregated into the fingerprint (aggregation spans 1 second by default, to accomodate time desynchronisation issues on the nodes)\n",
    "* `fr_values`: fingerprint, represented as a dictionary  {`node_id`: `signal_strength`, ...}. If a node ID is not present,  you can assume that its associated signal strength is `-100`\n",
    "* `fr_mac_address_id`: unique id of the mac address of the phone that emitted the corresponding probe request\n",
    "* `zo_name`: zone name (class to be predicted). Zone names are strings containing numbers, and do not have a direct semantic meaning.\n",
    "\n",
    "Some statistics about the dataset:\n",
    "\n",
    "* `343449` unique fingerprints (rows in the CSV file, excluding header line)\n",
    "* `19` unique mac address ids. That means that 19 different phones where used to collect the dataset\n",
    "* `449` unique zones\n",
    "* `261` unique nodes\n",
    "* On average, each node is present in `23224` fingerprints\n",
    "\n",
    "## Problem\n",
    "\n",
    "Given a fingerprint, predict its corresponding zone with a classifier.\n",
    "The dataset can be used for training and testing the classifier.\n",
    "Assess your solution with cross-validation by reporting results for confusion matrix, precision, recall and F1 score.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submit your solution by emailing us the link to a public GitHub repository, containing the solution as a Jupyter notebook and any additional files you deem necessary.\n",
    "Do not include the files contained in this archive in the public repository.\n",
    "You must use Python 3.x for your solution.\n",
    "We evaluate your submission by scoring it on these questions:\n",
    "\n",
    "* Is the candidate following the provided instructions?\n",
    "* Is there an accompanying README.md file, that describes the contents of the submission?\n",
    "* Is it well written using Markdown title(s), lists, and punctuation? any typos?\n",
    "* Is it well structured, with a title, introduction, conclusions, sections, and explanatory comments?\n",
    "* Is it easy to follow, with a clear and logical flow? Can I comprehend it without spending lots of energy?\n",
    "* Last but not least: How good are results?\n",
    "\n",
    "If you have any questions, feel free to approach us directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garret's Solution\n",
    "\n",
    "## Approach\n",
    "I chose five candidate classification algorithms. Four of the algorithms use decision tree estimators to predict labels. Decision trees were predominantely chosen as these estimators can natively handle multiclass outcomes with a high number of level (e.g. 449 unique zones in current data) without requiring one-versus-all, which would be computationally expensive for this number of levels. The fifth algorithm is a Support Vector Machines (SVM) that was chosen as it performs well across a diversity of problems. \n",
    "\n",
    "The models are as follows:\n",
    "\n",
    "1. Extra Trees:  computationally cheap algorithm that randomly chooses cut points (lower variance), as opposed to Random Forests that estimated optimum tree cut points and is therefore computationally more expensive (lower bias).\n",
    "Random Forests: a simple to use algorithm that requires little tuning.\n",
    "\n",
    "\n",
    "2. Random Forests: ensemble method of 'bagging' of decision trees, which averages of numerous randomized (in terms of features selection used for estimated fit) tree fits, and this reduces the influence of overfitting trees for a more accurate and generalizable estimator.\n",
    "\n",
    "\n",
    "3-4. Extreme Gradient Boosted Trees (including both Adaboost and XGB implementations of this algorithm): instead of averaging decision trees made in parallel using bagging as per Random Forests, this algorithm builds trees sequentially and therefore learns iteratively.\n",
    "\n",
    "\n",
    "5. SVM: \n",
    "\n",
    "## Steps\n",
    "Description of steps in numbered sections in analysis (steps 8-10 done twice for each model):\n",
    "\n",
    "\n",
    "1. Imported data from csv to pandas dataframe, and removed all columns except \n",
    "\n",
    "\n",
    "2. Imputed values of non-represented nodes (-100).\n",
    "\n",
    "\n",
    "3. Converted zone codes to numbers in a continuous sequence for prediction labels.\n",
    "\n",
    "\n",
    "4. Subsampled to 20% of total data set (equally from each zone) to save time for evaluating models during parameter tuning.\n",
    "\n",
    "\n",
    "5. Removed features that aren't likely to inform predictions of zone location (assuming new fingeprints will differ in the id and time from training data).\n",
    "\n",
    "\n",
    "6. Performed matrix algebra to end up with a sparse matrix (zeros where -100s were) to allow memory compression for faster training.\n",
    "\n",
    "\n",
    "7. Split data into train and test sets.\n",
    "\n",
    "\n",
    "8. Tuned parameters using grid search.\n",
    "\n",
    "\n",
    "9. Evaluated and compared models based on requested metrics.\n",
    "\n",
    "\n",
    "10. Conclusions on model comparisons and future directions\n",
    "\n",
    "## Comments\n",
    "* Models were run on an AWS EC2 c8.xlarge instance. Even with this high memory capacity, model training times were very slow (~1.5 hours). For this reason, several model types (e.g. Random Forest, SVM) returned memory errors, even when trained on small fractions of the data. This was partially fixed by compressing the sparse data matrix. Training times were further speeded up by conducted grid search using only 5-fold CV and on only 20% of the data set, sampled equally from each zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "# import data\n",
    "path = os.getcwd()\n",
    "df_dir = os.path.join(os.path.sep.join(path.split(os.path.sep)),'data')\n",
    "df = pd.read_csv(os.path.join(df_dir,'fingerprints_gt_ver3.csv'), index_col = False, quotechar = '\"') \n",
    "\n",
    "# remove double quotes around dict values\n",
    "conv = dict(fr_values=ast.literal_eval)\n",
    "df = pd.read_csv(os.path.join(df_dir,'fingerprints_gt_ver3.csv'),  converters=conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Impute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to unpack dictionary keys as pandas columns and impute missing values\n",
    "def unpack(df, column, fillna=None):\n",
    "    \"\"\"\n",
    "    This function goes through the dictionary values in the fr_values column\n",
    "    and unpacks them into new columns with key value as column name.\n",
    "    \"\"\"\n",
    "    ret = None\n",
    "    if fillna is None:\n",
    "        ret = pd.concat([df, pd.DataFrame((d for idx, d in df[column].iteritems()))], axis=1)\n",
    "        del ret[column]\n",
    "    else:\n",
    "        ret = pd.concat([df, pd.DataFrame((d for idx, d in df[column].iteritems())).fillna(fillna)], axis=1)\n",
    "        del ret[column]\n",
    "    return ret\n",
    "\n",
    "# run function\n",
    "df = unpack(df, 'fr_values', fillna=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert zone codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode zone names as numeric\n",
    "df['zo_name'] = df['zo_name'].str.replace('[^\\d.]', '')\n",
    "\n",
    "# factorise and record labels for decoding later\n",
    "cat_columns = ['zo_name']\n",
    "label_list = []\n",
    "\n",
    "for column in cat_columns:\n",
    "    \"\"\"\n",
    "    This function converts column levels into sequenital and factorized \n",
    "    categorical levels, keeping original values in dict for later decoding.\n",
    "    \"\"\"\n",
    "\n",
    "    df[column] = df[column].astype('category')\n",
    "    label_list.append(dict( enumerate(df[column].cat.categories) ))\n",
    "    df[column] = df[column].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Subsample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "fraction = .25\n",
    "df = df.groupby('zo_name', group_keys=False).apply(lambda x: x.sample(frac=fraction))\n",
    "y = df.zo_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['zo_name','fr_mac_address_id','fr_observation_time'], axis=1, inplace=True)\n",
    "X = df.astype('int8').values\n",
    "X = X+100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sparse matrix compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "X = csr_matrix(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Split train-test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics  \n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# class EstimatorSelectionHelper:\n",
    "#     \"\"\"\n",
    "#     This is a wrapper that takes in parameter ranges and selected \n",
    "#     models, performs grid search, and outputs model scores.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, models, params):\n",
    "#         if not set(models.keys()).issubset(set(params.keys())):\n",
    "#             missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "#             raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "#         self.models = models\n",
    "#         self.params = params\n",
    "#         self.keys = models.keys()\n",
    "#         self.grid_searches = {}\n",
    "\n",
    "#     def fit(self, X, y, cv=3, n_jobs=18, verbose=1, scoring=None, refit=False):\n",
    "#         for key in self.keys:\n",
    "#             print(\"Running GridSearchCV for %s.\" % key)\n",
    "#             model = self.models[key]\n",
    "#             params = self.params[key]\n",
    "#             gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "#                               verbose=verbose, scoring=scoring, refit=refit,\n",
    "#                               return_train_score=True)\n",
    "#             gs.fit(X,y)\n",
    "#             self.grid_searches[key] = gs    \n",
    "\n",
    "#     def score_summary(self, sort_by='mean_score'):\n",
    "#         def row(key, scores, params):\n",
    "#             d = {\n",
    "#                  'estimator': key,\n",
    "#                  'min_score': min(scores),\n",
    "#                  'max_score': max(scores),\n",
    "#                  'mean_score': np.mean(scores),\n",
    "#                  'std_score': np.std(scores),\n",
    "#             }\n",
    "#             return pd.Series({**params,**d})\n",
    "\n",
    "#         rows = []\n",
    "#         for k in self.grid_searches:\n",
    "#             print(k)\n",
    "#             params = self.grid_searches[k].cv_results_['params']\n",
    "#             scores = []\n",
    "#             for i in range(self.grid_searches[k].cv):\n",
    "#                 key = \"split{}_test_score\".format(i)\n",
    "#                 r = self.grid_searches[k].cv_results_[key]        \n",
    "#                 scores.append(r.reshape(len(params),1))\n",
    "\n",
    "#             all_scores = np.hstack(scores)\n",
    "#             for p, s in zip(params,all_scores):\n",
    "#                 rows.append((row(k, s, p)))\n",
    "\n",
    "#         df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "#         columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "#         columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "#         return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# models1 = {\n",
    "#     'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "#     'RandomForestClassifier': RandomForestClassifier(),\n",
    "#     'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "#     'XGBClassifier': XGBClassifier(),\n",
    "#     'SVC': SVC()\n",
    "# }\n",
    "\n",
    "# params1 = {\n",
    "#     'ExtraTreesClassifier': { 'n_estimators': [16, 32] },\n",
    "#     'RandomForestClassifier': { 'n_estimators': [16, 32] },\n",
    "#     'AdaBoostClassifier':  { 'n_estimators': [16, 32] },\n",
    "#     'XGBClassifier': { 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },\n",
    "#     'SVC': [\n",
    "#         {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1]},\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "# helper1.fit(X_train, y_train, scoring='f1_macro', n_jobs=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper1.score_summary(sort_by='max_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Model evaluations\n",
    "\n",
    "\n",
    "| estimator\t| min_score\t| mean_score | max_score | std_score | C |kernel | learning_rate | n_estimators |\n",
    "|-----------|-----------|------------|-----------|-----------|---|-------|---------------|--------------|\n",
    "| 3\t| RandomForestClassifier\t| 0.655098\t| 0.657277\t| 0.658536\t| 0.00154728\t| NaN\t| NaN\t| NaN\t| 32 |\n",
    "| 10  | SVC\t| 0.622443\t| 0.623999\t| 0.626721\t| 0.00193167\t| 0.001\t| linear\t| NaN\t| NaN|\n",
    "| 2\t|  RandomForestClassifier\t| 0.618987\t| 0.620894\t| 0.62456\t| 0.00259273\t| NaN\t| NaN\t| NaN\t| 16 |\n",
    "| 1\t|  ExtraTreesClassifier\t| 0.611042\t| 0.615962\t| 0.623342\t| 0.00531411\t| NaN\t| NaN\t| NaN\t| 32 |\n",
    "| 11 | SVC\t| 0.596674\t| 0.597627\t| 0.598637\t| 0.000802158\t| 0.01\t| linear\t| NaN\t| NaN |\n",
    "| 0\t| ExtraTreesClassifier\t| 0.570454\t| 0.580172\t| 0.589435\t| 0.00775566\t| NaN\t| NaN\t| NaN\t| 16 |\n",
    "| 12 | SVC\t| 0.584213\t| 0.585304\t| 0.586462\t| 0.000919383\t| 0.1\t| linear\t| NaN\t| NaN |\n",
    "| 7\t| XGBClassifier\t| 0.0172699\t| 0.035711\t| 0.049727\t| 0.0136149\t| NaN\t| NaN\t| 0.8\t| 32 |\n",
    "| 9\t| XGBClassifier\t| 0.015628\t| 0.0305242\t| 0.0469324\t| 0.0128246\t| NaN\t| NaN\t| 1\t| 32 |\n",
    "| 6\t| XGBClassifier\t| 0.017612\t| 0.0324528\t| 0.0448098\t| 0.0112415\t| NaN\t| NaN\t| 0.8\t| 16 |\n",
    "| 8\t| XGBClassifier\t| 0.0112355\t| 0.0223191\t| 0.0374607\t| 0.0110842\t| NaN\t| NaN\t| 1\t| 16 |\n",
    "| 4\t| AdaBoostClassifier\t| 0.00295974\t| 0.00417767\t| 0.00584784\t| 0.00122166\t| NaN\t| NaN\t| NaN\t| 16 |\n",
    "| 5\t| AdaBoostClassifier\t| 0.00295974\t| 0.00369243\t| 0.00450662\t| 0.000634136\t| NaN\t| NaN\t| NaN\t| 32 |\n",
    "\n",
    "\n",
    "We can see from the table \n",
    "\n",
    "While Gradient Boosted Trees algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(__doc__)\n",
    "# import itertools\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "#     \"\"\"\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalization can be applied by setting `normalize=True`.\n",
    "#     \"\"\"\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, format(cm[i, j], fmt),\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# np.set_printoptions(precision=2)\n",
    "\n",
    "# # Plot non-normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization')\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier as xgb\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# learning_rate = [0.0001, 0.01, 1]\n",
    "# param_grid = dict(learning_rate=learning_rate)\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "# clf_xgb = xgb(nthread=18, max_depth=2, silent=False, \n",
    "#               num_boost_round = 999, early_stopping_rounds=5,\n",
    "#               objective='multi:softmax', num_class=449, \n",
    "#               eval_metric='mlogloss')\n",
    "# grid_search = GridSearchCV(clf_xgb, param_grid, cv=kfold.split(X_train, y_train), n_jobs=-1)\n",
    "# grid_result = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "\n",
    "# clf_rf = rf(n_jobs=-1) #, max_features='sqrt', oob_score = True) \n",
    " \n",
    "# # Use a grid over parameters of interest\n",
    "# param_grid = {\"n_estimators\" : [50,100,200]}\n",
    " \n",
    "# cv_rfc = GridSearchCV(estimator=clf_rf, param_grid=param_grid, cv = 5)\n",
    "# cv_rfc.fit(X_train, y_train)\n",
    "# print(cv_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier as xgb\n",
    "# clf_xgb = xgb(n_estimators=100,nthreads=18,max_depth=2,eta=0.1, silent=False, \n",
    "#               num_boost_round = 999, early_stopping_rounds=10,\n",
    "#               objective='multi:softmax', num_class=449, \n",
    "#               eval_metric='mlogloss').fit(X_train,y_train)\n",
    "\n",
    "# # make predictions for test data\n",
    "# y_pred = clf_xgb.predict(X_test)\n",
    "# print(\"Accuracy: %.2f%%\" % (metrics.accuracy_score(y_test, y_pred) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_rf = RandomForestClassifier(n_jobs=18,max_features='sqrt', oob_score = True) \n",
    " \n",
    "# # Use a grid over parameters of interest\n",
    "# param_grid = {\"n_estimators\" : [128,256]}\n",
    " \n",
    "# cv_rf = GridSearchCV(estimator=clf_rf, param_grid=param_grid, cv = 3)\n",
    "# cv_rf.fit(X_train, y_train)\n",
    "# print(cv_rf.best_params_)\n",
    "#print(cv_rf.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit xtra\n",
    "clf_rf = RandomForestClassifier(n_estimators=256,n_jobs=18)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_predict = clf_rf.predict(X_test)\n",
    "\n",
    "print(\"accuracy: \", metrics.accuracy_score(y_test, y_predict))  \n",
    "print(\"precision: \", metrics.precision_score(y_test, y_predict, average='macro'))  \n",
    "print(\"recall: \", metrics.recall_score(y_test, y_predict, average='macro'))  \n",
    "print(\"f1: \", metrics.f1_score(y_test, y_predict, average='macro'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit RF\n",
    "# clf_xtra = RF(n_estimators=100)\n",
    "# clf_xtra.fit(X_train, y_train)\n",
    "# y_predict = clf_xtra.predict(X_test)\n",
    "\n",
    "# print(\"accuracy: \", metrics.accuracy_score(y_test, y_predict))  \n",
    "# print(\"precision: \", metrics.precision_score(y_test, y_predict, average='macro'))  \n",
    "# print(\"recall: \", metrics.recall_score(y_test, y_predict, average='macro'))  \n",
    "# print(\"f1: \", metrics.f1_score(y_test, y_predict, average='macro'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Hyperparameter tunining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# def svc_param_selection(X, y, nfolds):\n",
    "#     Cs = [0.001, 0.0001, 0.00001,]\n",
    "#     param_grid = {'C': Cs}\n",
    "#     grid_search = GridSearchCV(svm.LinearSVC(), param_grid, cv=nfolds)\n",
    "#     grid_search.fit(X, y)\n",
    "#     grid_search.best_params_\n",
    "#     return grid_search.best_params_\n",
    "\n",
    "# best_parameters = svc_param_selection(X_test, y_test, 10)\n",
    "# print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVC as SVC\n",
    "# #from sklearn.ensemble import RandomForestClassifier as RF\n",
    "# #from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn import metrics  \n",
    "\n",
    "# # fit SVC\n",
    "# clf_svc = SVC(dual=False,C=.0001) # from scikit-learn docs: Prefer dual=False when n_samples > n_features\n",
    "# clf_svc.fit(X_train, y_train)\n",
    "\n",
    "# # fit RF\n",
    "# clf_rf = RF(n_estimators=100)\n",
    "# clf_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict = clf_svc.predict(X_test)\n",
    "\n",
    "# print(\"accuracy: \", metrics.accuracy_score(y_test, y_predict))  \n",
    "# print(\"precision: \", metrics.precision_score(y_test, y_predict, average='macro'))  \n",
    "# print(\"recall: \", metrics.recall_score(y_test, y_predict, average='macro'))  \n",
    "# print(\"f1: \", metrics.f1_score(y_test, y_predict, average='macro'))  \n",
    "# #print(\"area under curve (auc): \", metrics.roc_auc_score(y_test, y_predict, average='macro'))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
